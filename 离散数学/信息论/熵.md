
## 信息论

**自信息量** 描述某事件带来的信息量, 概率越小信息量越大. 设 X 是离散型随机变量, 概率分布为 $p(x)=P(X=x), x\in X$. X 的自信息量为 $$I(x)=-\log_{2}P(x)$$

**信息熵 (entropy)** 描述一个随机变量的不确定性. X 的信息熵为 $$H(x)=-\sum_{x\in X}P(x)\cdot \log_{2}P(x)$$ 约定 $0\log0=0$, 熵单位为二进制位bit. **当所有码字出现概率相等, 即对所有结果没有偏见时, 信息熵最大**

### 联合熵 

Joint Entropy, 联合熵指一对随机变量平均所需要的信息量. 设 $X, Y$ 为一对离散随机变量, $X, Y\sim p(x, y)$, 联合熵定义为: $$H(X, Y)=-\sum_{x\in X}\sum_{y\in Y}p(x,y)\cdot \log_{2}p(x,y)$$

### 条件熵

Conditional Entropy 条件熵定义为: (给定随机变量X)
$$\begin{align}
H(Y\vert X)&=\sum_{x\in X}p(x)H(Y\vert X=x) \\
&=\sum_{x\in X}p(x)\left[ -\sum_{y\in Y}p(y\vert x)\cdot \log_{2}p(y\vert x) \right] \\
&=-\sum_{x\in X}\sum_{y\in Y}p(x,y)\log_{2}p(y\vert x) \\
&=-\sum_{x\in X}\sum_{y\in Y}p(x,y)[\log_{2} p(x)+\log_{2}p(y\vert x)] \\
&=-\sum_{x\in X}\sum_{y\in Y}p(x,y)\log_{2} p(x)-\sum_{x\in X}\sum_{y\in Y}p(x,y)\log_{2} p(y\vert x)  \\
&=-\sum_{x\in X}p(x)\log_{2}p(x)--\sum_{x\in X}\sum_{y\in Y}p(x,y)\log_{2} p(y\vert x) \\
&= H(X)+H(Y\vert X)
\end{align}$$

无记忆信息熵 ( 0阶熵, $H_{0}(\cdot)$ ): 各像素的灰度值是相互独立的.

**条件熵**: 对于记忆信源, 假设某一像素灰度值和前一个像素灰度级相关.
$$H\left( \frac{W_{i}}{W_{i-1}} \right)=-\sum^{M}_{k=1}\sum^{M}_{k=1}P(W_{i}, W_{i-1})\cdot \log_{2}P\left( \frac{W_{i}}{W_{i-1}} \right)$$
而其中 $P(W_{i}, W_{i-1})=P(W_{i})P\left( \frac{W_{i}}{W_{i-1}} \right)$. 称为一阶熵 $H_{1}(\cdot)$. 

可知 $H_{0}(\cdot)>H_{1}(\cdot)>H_{2}(\cdot)>\dots$

### 熵率

一般地, 对于长度为n的信息, 每个字的熵为 $$H_{rate}=\frac{1}{n}H(X_{1n})=-\frac{1}{n}\sum_{x_{1n}}p(x_{1n})\cdot \log_{2}p(x_{1n})$$ 称为熵率 (Entropy Rate). 其中 变量 $X_{1n}$ 表示随机变量序列 $(X_{1}, \dots, X_{n})$,  $x_{1n}=x_{1},\dots,x_{n}$ 或 $x_{1}^{n}=(x_{1},\dots,x_{n})$

### 相对熵

相对熵 (relative entropy) 也称为KL距离 (Kullback-Leibler Divergence). 两个概率分布 $p(x)$ 和 $q(x)$ 的相对熵定义为: $$D(p\Vert q)=\sum_{x\in X}p(x)\log_{2} \frac{p(x)}{q(x)}$$ 其中 $0\log_{2}(0/q)=0,\,p\log(p/0)=\infty$.