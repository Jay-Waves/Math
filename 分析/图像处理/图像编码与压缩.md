### 图像编码分类:

1. 信息保持编码 (无损编码): 不丢失信息
2. 保真度编码 (有损编码): 压缩人眼感知不到的信息
3. 特征编码 (感兴趣区域编码): 仅保留供机器分析的特征

传输过程:  
`图像编码` -> `通道编码` -> `通道` -> `通道解码` -> `图像解码`

图像编码过程:  
`图像变换` -> `量化` -> `熵编码`

图像解码:  
`符号编码` -> `图像反变换`

## 1 信息论

**自信息量** 描述某事件带来的信息量, 概率越小信息量越大  
设图像灰度集为 $(W_{1},W_{2},\dots,W_{M})$, 概率分别为 $P_{1}, P_{2}, \dots, P_{M}$, 其自信息量为 $I(W_{K})=-\log_{2}P_{k}(W_{K})$

**信息熵 (entropy)** 描述一个随机变量的不确定性.  
$$H=\sum^{M}_{k=1}P_{K}(W_{k})\cdot I(W_{k})=-\sum^{M}_{k=1}P_{k}\log_{2}P_{k}$$

**当所有码字出现概率相等, 即对所有结果没有偏见时, 信息熵最大**

### 条件熵

无记忆信息熵 ( 0阶熵, $H_{0}(\cdot)$ ): 各像素的灰度值是相互独立的.

**条件熵**: 对于记忆信源, 假设某一像素灰度值和前一个像素灰度级相关.
$$H\left( \frac{W_{i}}{W_{i-1}} \right)=-\sum^{M}_{k=1}\sum^{M}_{k=1}P(W_{i}, W_{i-1})\cdot \log_{2}P\left( \frac{W_{i}}{W_{i-1}} \right)$$
而其中 $P(W_{i}, W_{i-1})=P(W_{i})P\left( \frac{W_{i}}{W_{i-1}} \right)$. 称为一阶熵 $H_{1}(\cdot)$. 

可知 $H_{0}(\cdot)>H_{1}(\cdot)>H_{2}(\cdot)>\dots$

> 详见 [熵](../../离散数学/信息论/编码.md), 以及 [熵](../../离散数学/信息论/熵.md)

## 2 具体编码方法

### 哈夫曼 (Huffman) 编码

通过构建二叉树来编码: (自底向下)
1. 为每个符号创建一个节点, 权值为其出现概率.
2. 按权值从小到大排序节点, 使用优先队列存储.
3. 构建树:
	- 选出权值最小两个节点
	- 创建新内部节点, 作为这两个节点父节点, 其权值为两子节点权值和.
	- 将新节点添加回优先队列, 重新排序
	- 重复该过程, 直至只剩根节点.
4. 分配码字, 从根开始, 向左分配0, 向右分配1.

使用二叉树译码, 顺序读入二进制编码, 从root开始, 对于每个编码:
- 读1进入左子树
- 读0进入右子树
- 若到达叶子节点, 则译出一个符号.

### 香农 (Shannon-Fano) 编码

香农编码思想是通过累加概率, 为每个符号分配一个 $[0,1)$ 的概率区间.

1. 将灰度级按出现概率从大到小排列
2. 计算码字长度 $-\log_{2}P_{i} \le \beta_{i}<-\log_{2}P_{i}+1$
3. 计算累加概率: $$\begin{align}
a_{1}&=0 \\
a_{2}&=P_{1} \\
a_{3}&=P_{2}+a_{2} =P_{2}+P_{1} \\
a_{4}&=P_{3}+a_{3} =P_{3}+P_{2}+P_{1} \\
\vdots \\
a_{i}&=P_{i-1}+P_{i-2}+\dots+P_{1}
\end{align}$$
4. 将 $a_{i}$ 十进制转二进制
5. 去除多余尾数, 补至 $beta_{i}$ 长.

**香农编码效率低于哈夫曼编码**

### 算术编码

算术编码思想是选定一个区间代表整个消息序列, 而不是寻求符号和码字的一一对应. **信源均匀情况下, 传输的符号概率表体积很小, 其编码效率甚至可高于哈夫曼编码**

举个例子来理解其编码过程: 假设四个符号 $A, B, C, D$, 概率分别为 $0.4, 0.3, 0.2, 0.1$. 此时对消息 "AB" 进行编码:
- 初始化区间 $[0, 1)$
- 处理消息符号A, 将区间划分为:$$\begin{align}
A:& [0, 0.4) \\
B:& [0.4, 0.7)  \\
C:& [0.7, 0.9)  \\
D:& [0.9, 1.0)
\end{align}$$ 此后选取 $[0, 0.4)$ 为新编码空间.
- 处理消息符号B, 将 $[0, 0.4)$ 划分为:$$\begin{align}
A:& [0, 0.16) \\
B:& [0.16, 0.28)  \\
C:& [0.28, 0.36) \\
D:& [0.36, 0.4)
\end{align}$$ 此后选取 $[0.36, 0.4)$ 为新编码空间.
- 消息处理完毕, 消息"AB"的编码区间为 $[0.36, 0.4)$, 区间中任意实数都可以唯一标识这个消息"AB"


## 3 预测和量化

预测法编码分类:
- 线性预测: 差值脉冲编码调制法 (DPCM)
- 非线性预测:

- 帧内预测法: 在同一图像内进行
- 帧内预测法: 在多幅图像之间进行

### 预测法编码

基本思想是用已存在像素值拟合线性函数, 用来预测未来的像素, 即 $\hat{x}_{n+1}=a_{1}x_{1}+a_{2}x_{2}+\dots a_{n}x_{n}$ 预测, 预测误差为 $e_{n+1}=x_{n+1}-\hat{x}_{n+1}$. 预测法编码通常对**预测误差编码**, 而不是对信号本身编码.


预测和量化

图像降质:
- 斜率过载引起的黑白边沿模糊, 分辨率降低
- 颗粒噪声: 量化下界过大
- 假轮廓: 量化间隔过大, 不够精细
- 边沿忙乱: (前后两张图边沿波动) 
- 误码扩散: 